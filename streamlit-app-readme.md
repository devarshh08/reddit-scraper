# Reddit Scraper & Data Cleaner - Streamlit App

A comprehensive web application that combines Reddit scraping and data cleaning functionality in a single, user-friendly interface.

## ğŸš€ Features

### Reddit Scraper
- **Subreddit Selection**: Scrape any public subreddit
- **Sorting Options**: Choose from `new`, `hot`, `top`, or `controversial` posts
- **Time Filters**: For top/controversial posts, filter by `day`, `week`, `month`, `year`, or `all`
- **Post Limit**: Scrape 1-1000 posts (customizable)
- **Keyword Filtering**: Filter posts containing specific keywords in title, content, or comments
- **Progress Tracking**: Real-time progress bar during scraping
- **Download Options**: Export data as JSON or cleaned text format

### Data Cleaner
- **File Upload**: Upload JSON files from the Reddit scraper
- **Data Validation**: Automatic validation of JSON structure
- **Clean Formatting**: Convert JSON to readable text format
- **Preview**: View data summary and preview before cleaning
- **Download**: Export cleaned data as text file

## ğŸ“‹ Requirements

- Python 3.7+
- Reddit API credentials (already included in the app)

## ğŸ”§ Installation

1. **Clone the repository** (if not already done):
   ```bash
   git clone <repository-url>
   cd reddit-scraper
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   - Copy `.env.example` to `.env`:
     ```bash
     cp .env.example .env
     ```
   - Edit `.env` and add your Reddit API credentials:
     ```
     CLIENT_ID=your_client_id_here
     CLIENT_SECRET=your_client_secret_here
     USER_AGENT=script:your-app-name:v1.0 (by u/your_username)
     ```

## ğŸƒâ€â™‚ï¸ Running the App

1. **Start the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

2. **Access the app**:
   - The app will automatically open in your default browser
   - If not, navigate to `http://localhost:8501`

## ğŸ“– How to Use

### Reddit Scraper Page

1. **Select "Reddit Scraper"** from the sidebar
2. **Enter subreddit name** (without "r/", e.g., "python", "AskReddit")
3. **Choose sorting method**:
   - `new`: Latest posts first
   - `hot`: Currently trending posts
   - `top`: Top posts (requires time filter)
   - `controversial`: Most controversial posts (requires time filter)
4. **Set time filter** (for top/controversial):
   - `day`, `week`, `month`, `year`, or `all`
5. **Specify number of posts** (1-1000)
6. **Add keywords** (optional):
   - Enter comma-separated or line-separated keywords
   - Only posts containing these keywords will be scraped
7. **Click "Start Scraping"**
8. **Download results**:
   - JSON format (raw data)
   - Cleaned text format (readable)

### Data Cleaner Page

1. **Select "Data Cleaner"** from the sidebar
2. **Upload a JSON file** (generated by the Reddit scraper)
3. **Review data summary** and preview
4. **Click "Clean Data"**
5. **Preview cleaned text** format
6. **Download cleaned text file**

## ğŸ“ File Structure

```
reddit-scraper/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ scraper-main.py            # Original CLI scraper
â”œâ”€â”€ data-cleaner.py            # Original CLI data cleaner
â”œâ”€â”€ scraper-readme.md          # Original scraper documentation
â”œâ”€â”€ data-cleaner-readme.md     # Original cleaner documentation
â””â”€â”€ streamlit-app-readme.md    # This file
```

## ğŸ”‘ API Credentials

You need to set up your own Reddit API credentials:

1. **Create a Reddit app**:
   - Go to https://www.reddit.com/prefs/apps
   - Click "Create App" or "Create Another App"
   - Choose "script" as the app type
   - Note down your `CLIENT_ID` and `CLIENT_SECRET`

2. **Configure environment variables**:
   - Copy `.env.example` to `.env`
   - Fill in your credentials in the `.env` file:
     ```
     CLIENT_ID=your_client_id_here
     CLIENT_SECRET=your_client_secret_here
     USER_AGENT=script:your-app-name:v1.0 (by u/your_username)
     ```

## âš ï¸ Important Notes

- **Rate Limiting**: Reddit API has rate limits. Large scraping operations may take time
- **Public Subreddits Only**: Private subreddits cannot be scraped
- **Content Policy**: Respect Reddit's content policy and terms of service
- **File Size**: Large datasets may take time to process and download

## ğŸ› Troubleshooting

### Common Issues:

1. **"Subreddit not found" error**:
   - Check spelling of subreddit name
   - Ensure subreddit is public

2. **Slow scraping**:
   - Reddit API rate limits may cause delays
   - Reduce number of posts for faster results

3. **App won't start**:
   - Ensure all dependencies are installed: `pip install -r requirements.txt`
   - Check Python version (3.7+ required)

4. **JSON upload errors**:
   - Ensure file was generated by the Reddit scraper
   - Check file is valid JSON format

## ğŸš€ Deployment Options

### Local Development
```bash
streamlit run app.py
```

### Streamlit Cloud
1. Push code to GitHub repository
2. Connect repository to Streamlit Cloud
3. Deploy with one click

### Heroku
1. Add `Procfile`: `web: streamlit run app.py --server.port=$PORT --server.address=0.0.0.0`
2. Configure buildpacks for Python
3. Deploy to Heroku

## ğŸ“ Support

For issues or questions, please check the documentation or create an issue in the repository.

## ğŸ“„ License

This project is open source. Please respect Reddit's API terms of service and content policy when using this tool.